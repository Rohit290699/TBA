{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23099098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords #you can remove stop words for speed\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47511157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was the best of times\n",
      "It was the worst of times\n",
      "It was the age of wisdom\n",
      "It was the age of foolishness\n",
      "What is the importance of age\n",
      "This is the best example.\n"
     ]
    }
   ],
   "source": [
    "file = open(\"C:\\\\Users\\\\Venkat Dyagala\\\\Desktop\\\\TBA_Class\\\\Text1.txt\", \"r\")\n",
    "#This file contains one paragraph of multiple sentences\n",
    "filedata = file.readlines()\n",
    "article = filedata[0].split(\". \") #Just do the first paragraph\n",
    "\n",
    "sentences = []\n",
    "for sentence in article:\n",
    "    print(sentence)\n",
    "    sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22c1ac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2 ):\n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "     # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "          vector1[all_words.index(w)] += 1\n",
    "     # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "          vector2[all_words.index(w)] += 1\n",
    "    return 1 - cosine_distance(vector1, vector2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c08a2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smilarity matrix \n",
      " [[0.         0.83333333 0.66666667 0.66666667 0.33333333 0.36514837]\n",
      " [0.83333333 0.         0.66666667 0.66666667 0.33333333 0.18257419]\n",
      " [0.66666667 0.66666667 0.         0.83333333 0.5        0.18257419]\n",
      " [0.66666667 0.66666667 0.83333333 0.         0.5        0.18257419]\n",
      " [0.33333333 0.33333333 0.5        0.5        0.         0.36514837]\n",
      " [0.36514837 0.18257419 0.18257419 0.18257419 0.36514837 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    "for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "             if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue \n",
    "             similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2])\n",
    "\n",
    "print(\"Smilarity matrix \\n\", similarity_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "244bae14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores {0: 0.19306449754902083, 1: 0.18095893645850156, 2: 0.1911855225552033, 3: 0.1911855225552033, 4: 0.14434636291527997, 5: 0.09925915796679063}\n"
     ]
    }
   ],
   "source": [
    "# Step 3 - Rank sentences in similarity martix\n",
    "sentence_similarity_graph = nx.from_numpy_array(similarity_matrix)\n",
    "scores = nx.pagerank(sentence_similarity_graph)\n",
    "print(\"scores\", scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f9962c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes of top ranked_sentence order are \n",
      "\n",
      " [(0.19306449754902083, ['It', 'was', 'the', 'best', 'of', 'times']), (0.1911855225552033, ['It', 'was', 'the', 'age', 'of', 'wisdom']), (0.1911855225552033, ['It', 'was', 'the', 'age', 'of', 'foolishness']), (0.18095893645850156, ['It', 'was', 'the', 'worst', 'of', 'times']), (0.14434636291527997, ['What', 'is', 'the', 'importance', 'of', 'age']), (0.09925915796679063, ['This', 'is', 'the', 'best', 'example.'])]\n"
     ]
    }
   ],
   "source": [
    "# Step 4 - Sort the rank and pick top sentences\n",
    "ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n",
    "print(\"Indexes of top ranked_sentence order are \\n\\n\", ranked_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ed9434",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5 - How many sentences to pick\n",
    "n = int(input(\"How many sentences do you want in the summary? \"))\n",
    "#n=2\n",
    "summarize_text = []\n",
    "for i in range(n):\n",
    "      summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a7509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summarize Text: \\n\", \". \".join(summarize_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3b2023",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
