{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e524fe54",
   "metadata": {},
   "source": [
    "# # importing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23099098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords #you can remove stop words for speed\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f917a091",
   "metadata": {},
   "source": [
    "# splitting text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47511157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine there's no heaven\n",
      "It's easy if you try\n",
      "No hell below us\n",
      "Above us, only sky\n",
      "Imagine all the people livin' for today\n",
      "Imagine there's no countries\n",
      "It isn't hard to do\n",
      "Nothing to kill or die for and no religion, too\n",
      "Imagine all the people livin' life in peace\n",
      "You may say I'm a dreamer but I'm not the only one\n",
      "I hope someday you'll join us and the world will be as one\n",
      "Imagine no possessions\n",
      "I wonder if you can\n",
      "No need for greed or hunger\n",
      "A brotherhood of man\n",
      "Imagine all the people sharing all the world\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = open(r\"C:\\Users\\Venkat Dyagala\\Desktop\\TBA_Class\\TBA\\Text4.txt\", \"r\")\n",
    "#This file contains one paragraph of multiple sentences\n",
    "filedata = file.readlines()\n",
    "article = filedata[0].split(\". \") #Just do the first paragraph\n",
    "\n",
    "sentences = []\n",
    "for sentence in article:\n",
    "    print(sentence)\n",
    "    sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22c1ac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2 ):\n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "     # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "          vector1[all_words.index(w)] += 1\n",
    "     # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "          vector2[all_words.index(w)] += 1\n",
    "    return 1 - cosine_distance(vector1, vector2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81d74a0",
   "metadata": {},
   "source": [
    "# creating the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c08a2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smilarity matrix \n",
      " [[0.         0.         0.25       0.         0.18898224 0.75\n",
      "  0.         0.15811388 0.1767767  0.         0.         0.57735027\n",
      "  0.         0.20412415 0.         0.14433757 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.11952286 0.         0.\n",
      "  0.4        0.         0.         0.         0.        ]\n",
      " [0.25       0.         0.         0.         0.         0.25\n",
      "  0.         0.15811388 0.         0.         0.13867505 0.28867513\n",
      "  0.         0.20412415 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.13363062 0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.18898224 0.         0.         0.         0.         0.18898224\n",
      "  0.         0.11952286 0.6681531  0.10101525 0.10482848 0.21821789\n",
      "  0.         0.15430335 0.         0.65465367 0.        ]\n",
      " [0.75       0.         0.25       0.         0.18898224 0.\n",
      "  0.         0.15811388 0.1767767  0.         0.         0.57735027\n",
      "  0.         0.20412415 0.         0.14433757 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.14142136 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.15811388 0.         0.15811388 0.         0.11952286 0.15811388\n",
      "  0.14142136 0.         0.         0.         0.0877058  0.18257419\n",
      "  0.         0.38729833 0.         0.         0.        ]\n",
      " [0.1767767  0.         0.         0.         0.6681531  0.1767767\n",
      "  0.         0.         0.         0.09449112 0.09805807 0.20412415\n",
      "  0.         0.         0.         0.61237244 0.        ]\n",
      " [0.         0.11952286 0.         0.13363062 0.10101525 0.\n",
      "  0.         0.         0.09449112 0.         0.14824986 0.\n",
      "  0.11952286 0.         0.13363062 0.15430335 0.        ]\n",
      " [0.         0.         0.13867505 0.         0.10482848 0.\n",
      "  0.         0.0877058  0.09805807 0.14824986 0.         0.\n",
      "  0.12403473 0.         0.         0.24019223 0.        ]\n",
      " [0.57735027 0.         0.28867513 0.         0.21821789 0.57735027\n",
      "  0.         0.18257419 0.20412415 0.         0.         0.\n",
      "  0.         0.23570226 0.         0.16666667 0.        ]\n",
      " [0.         0.4        0.         0.         0.         0.\n",
      "  0.         0.         0.         0.11952286 0.12403473 0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.20412415 0.         0.20412415 0.         0.15430335 0.20412415\n",
      "  0.         0.38729833 0.         0.         0.         0.23570226\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.13363062 0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.14433757 0.         0.         0.         0.65465367 0.14433757\n",
      "  0.         0.         0.61237244 0.15430335 0.24019223 0.16666667\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    "for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "             if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue \n",
    "             similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2])\n",
    "\n",
    "print(\"Smilarity matrix \\n\", similarity_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac08e6a",
   "metadata": {},
   "source": [
    "# # Rank sentences in similarity martix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4149c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores {0: 0.09163334044231103, 1: 0.04408562199586315, 2: 0.05432094163511732, 3: 0.01779516045726271, 4: 0.09575693097837697, 5: 0.09163334044231103, 6: 0.014924507861294583, 7: 0.0653124759476629, 8: 0.08186947693779716, 9: 0.07522194125679456, 10: 0.05101030185038315, 11: 0.09222556696979164, 12: 0.05146059071771966, 13: 0.05778838761945695, 14: 0.01779516045726271, 15: 0.08787832873399998, 16: 0.009287925696594429}\n"
     ]
    }
   ],
   "source": [
    "sentence_similarity_graph = nx.from_numpy_array(similarity_matrix)\n",
    "scores = nx.pagerank(sentence_similarity_graph)\n",
    "print(\"scores\", scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1730a75f",
   "metadata": {},
   "source": [
    "# Sort the rank and pick top sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f9962c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes of top ranked_sentence order are \n",
      "\n",
      " [(0.09575693097837697, ['Imagine', 'all', 'the', 'people', \"livin'\", 'for', 'today']), (0.09222556696979164, ['Imagine', 'no', 'possessions']), (0.09163334044231103, ['Imagine', \"there's\", 'no', 'heaven']), (0.09163334044231103, ['Imagine', \"there's\", 'no', 'countries']), (0.08787832873399998, ['Imagine', 'all', 'the', 'people', 'sharing', 'all', 'the', 'world']), (0.08186947693779716, ['Imagine', 'all', 'the', 'people', \"livin'\", 'life', 'in', 'peace']), (0.07522194125679456, ['You', 'may', 'say', \"I'm\", 'a', 'dreamer', 'but', \"I'm\", 'not', 'the', 'only', 'one']), (0.0653124759476629, ['Nothing', 'to', 'kill', 'or', 'die', 'for', 'and', 'no', 'religion,', 'too']), (0.05778838761945695, ['No', 'need', 'for', 'greed', 'or', 'hunger']), (0.05432094163511732, ['No', 'hell', 'below', 'us']), (0.05146059071771966, ['I', 'wonder', 'if', 'you', 'can']), (0.05101030185038315, ['I', 'hope', 'someday', \"you'll\", 'join', 'us', 'and', 'the', 'world', 'will', 'be', 'as', 'one']), (0.04408562199586315, [\"It's\", 'easy', 'if', 'you', 'try']), (0.01779516045726271, ['Above', 'us,', 'only', 'sky']), (0.01779516045726271, ['A', 'brotherhood', 'of', 'man']), (0.014924507861294583, ['It', \"isn't\", 'hard', 'to', 'do']), (0.009287925696594429, ['\\n'])]\n"
     ]
    }
   ],
   "source": [
    "ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n",
    "print(\"Indexes of top ranked_sentence order are \\n\\n\", ranked_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c182d3e9",
   "metadata": {},
   "source": [
    "# How many sentences to pick\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76ed9434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many sentences do you want in the summary? 6\n"
     ]
    }
   ],
   "source": [
    "n = int(input(\"How many sentences do you want in the summary? \"))\n",
    "#n=2\n",
    "summarize_text = []\n",
    "for i in range(n):\n",
    "      summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b617a915",
   "metadata": {},
   "source": [
    "# printing the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27a7509c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize Text: \n",
      " Imagine all the people livin' for today. Imagine no possessions. Imagine there's no heaven. Imagine there's no countries. Imagine all the people sharing all the world. Imagine all the people livin' life in peace\n"
     ]
    }
   ],
   "source": [
    "print(\"Summarize Text: \\n\", \". \".join(summarize_text))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
